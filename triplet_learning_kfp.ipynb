{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-end triplet loss using Kubeflow pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This notebook is intended to be run from the Jupyterlab environment in a GKE-based Kubeflow cluster.  This started as a thought experiment on what an \"end-to-end\" ML pipeline looks like within a single notebook.  \n",
    "\n",
    "Kubeflow pipelines uses containers to execute steps of the pipeline; instead of developing container code separately outside the notebook, we explore ways to write code once inside the notebook, and have this be executable inside the notebook and/or built into Docker containers - all from the notebook interface.\n",
    "\n",
    "\n",
    "## Learning using triplet loss\n",
    "\n",
    "To prove out the concept, we try and implement an end-to-end ML pipeline for a \"triplet learning\" model.  The triplet loss model is trained using face images from the [PubFig dataset](http://www.cs.columbia.edu/CAVE/databases/pubfig/).  The trained model predicts an embedded representation of a face image, such that similar faces result in more similar embeddings.  This might be usefully applied for applications such as face clustering and recognition.\n",
    "\n",
    "Find out more about triplet loss [here](https://www.coursera.org/lecture/convolutional-neural-networks/triplet-loss-HuUtN).\n",
    "\n",
    "## Before you run\n",
    "Since we use Cloud Build directly in the notebook (hence avoiding the need for the \"docker\" binary being available inside Jupyter), the GKE worker nodes should be created with the `https://www.googleapis.com/auth/cloud-platform` scope.  The  Kubeflow user service account should also be granted `Cloud Build Service Account` IAM role, so that Cloud Build jobs can be submitted from the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo $GOOGLE_APPLICATION_CREDENTIALS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define custom magics\n",
    "\n",
    "We explore the use of Jupyter magics to easily work with Docker containers within the notebook environment. The following magics are defined:\n",
    "\n",
    "`%%containerize [docker-container-tag] [filename]`\n",
    "\n",
    "Adds the contents of the cell to a file named `[filename]`.  `[filename]` can be referenced within the container's Dockerfile (for example, `COPY [filename] /app`.\n",
    "\n",
    "`%%containerize_and_run [docker-container-tag] [filename]`\n",
    "\n",
    "Adds the contents of the cell to a file name `[filename]`, and additionally runs the cell's contents like normal.  This allows code to be executed within the notebook context, and avoids having to `docker build ... / docker run ...` code just for testing.\n",
    "\n",
    "`%%containerize_file [docker-container-tag] [source_filename] [container_filename]`\n",
    "\n",
    "Adds a file from the local file system to a container. This works just like `%%containerize`, except the contents of `source_filename` is used for the file instead of the cell's contents.\n",
    "\n",
    "`%cloud_build [docker-container-tag]`\n",
    "\n",
    "Submits a container build job to Cloud Build. If the job is successful, the container image will be available in Google Container Registry.\n",
    "\n",
    "`%container_build [docker-container-tag]`\n",
    "\n",
    "Uses a locally installed Docker to build the container.  Assumes that you have Docker installed (which the Jupyterlab environment does not).\n",
    "\n",
    "`%container_push [docker-container-tag]`\n",
    "\n",
    "Pushes the container to a Docker registry.  Assumes that you have Docker installed (which the Jupyterlab environment does not).\n",
    "\n",
    "`%container_run [docker-container-tag]`\n",
    "\n",
    "Runs the container using the locally-installed Docker.  Assumes that you have Docker installed (which the Jupyterlab environment does not).\n",
    "\n",
    "The following code defines how our custom magics work. **It is included directly in the notebook for demonstrative purposes**, but you may want to externalise this into a separate file and import it instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core import magic_arguments\n",
    "from IPython.core.magic import line_magic, cell_magic, line_cell_magic, Magics, magics_class\n",
    "\n",
    "@magics_class\n",
    "class ContainerMagics(Magics):\n",
    "    \n",
    "    def __init__(self, shell):\n",
    "        super(ContainerMagics, self).__init__(shell)        \n",
    "        self.containers = {}\n",
    "\n",
    "\n",
    "    def get_container(self, name):\n",
    "        if name in self.containers.keys():\n",
    "            return self.containers[name]\n",
    "\n",
    "        \n",
    "    def update_or_add_container(self, name, container):\n",
    "        self.containers[name] = container\n",
    "        \n",
    "    \n",
    "    class Container(object):\n",
    "        def __init__(self, name):\n",
    "            self.name = name\n",
    "            self.files = {}\n",
    "        \n",
    "        \n",
    "        def add_file(self, file_name, contents):\n",
    "            self.files[file_name] = contents\n",
    "            \n",
    "        \n",
    "        def add_from_file(self, source_file, dest_file):\n",
    "            with open(source_file, 'rb') as f:\n",
    "                contents = f.read()\n",
    "            self.files[dest_file] = contents.decode(\"utf-8\")\n",
    "            \n",
    "            \n",
    "        def has_dockerfile(self):\n",
    "            return 'Dockerfile' in self.files.keys()\n",
    "        \n",
    "        \n",
    "        def build(self, args, use_gcp=False):\n",
    "            import os\n",
    "            import logging\n",
    "            import subprocess\n",
    "            import tempfile\n",
    "            assert 'Dockerfile' in self.files.keys(), 'No Dockerfile defined'\n",
    "\n",
    "            print(\"Building container \\'{}\\' ...\".format(self.name))            \n",
    "            td = tempfile.mkdtemp()\n",
    "            for base_name in self.files.keys():\n",
    "                full_path = os.path.join(td, base_name)\n",
    "                with open(full_path, 'w') as f:\n",
    "                    f.write(self.files[base_name])\n",
    "                    print(\"==> Wrote {}\".format(full_path))\n",
    "                f.close()\n",
    "            \n",
    "            try:\n",
    "                if use_gcp:\n",
    "                    print(\"==> Submitting cloud build ...\")                    \n",
    "                    out = subprocess.check_output(['gcloud', 'builds', 'submit', '--tag', self.name, '.'], \n",
    "                                                  cwd=td, stderr=subprocess.STDOUT)\n",
    "                else:\n",
    "                    print(\"==> Running docker build ...\")                    \n",
    "                    out = subprocess.check_output(['docker', 'build', '-t', self.name, '.'], \n",
    "                                                  cwd=td, stderr=subprocess.STDOUT)\n",
    "                print(out.decode('utf-8'))\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                raise RuntimeError(\"Command '{}' return with error (code {}): {}\".format(e.cmd, e.returncode, e.output.decode('utf-8')))\n",
    "                \n",
    "            \n",
    "        def push(self, args):\n",
    "            import subprocess          \n",
    "            try:            \n",
    "                print(\"==> Running docker push ...\")\n",
    "                out = subprocess.check_output(['docker', 'push', self.name])\n",
    "                print(out.decode('utf-8'))\n",
    "            except subprocess.CalledProcessError as e:\n",
    "                raise RuntimeError(\"Command '{}' return with error (code {}): {}\".format(e.cmd, e.returncode, e.output.decode('utf-8')))\n",
    "            \n",
    "            \n",
    "        def run(self, args):\n",
    "            import subprocess\n",
    "            out = subprocess.check_output(['docker', 'run', self.name] + args)\n",
    "            print(out.decode('utf-8'))\n",
    "            \n",
    "        \n",
    "    @cell_magic    \n",
    "    def containerize(self, line='', cell=None):\n",
    "        tokens = line.split(' ')\n",
    "        \n",
    "        assert len(tokens) == 2, 'Expecting 2 arguments, got {}'.format(str(len(tokens)))\n",
    "        container_name, file_name = tokens[0], tokens[1]\n",
    "        \n",
    "        container = self.get_container(container_name)\n",
    "        if not container:\n",
    "            container = self.Container(container_name)\n",
    "            \n",
    "        container.add_file(file_name, cell)\n",
    "        self.update_or_add_container(container_name, container)\n",
    "        \n",
    "    @cell_magic    \n",
    "    def containerize_and_run(self, line='', cell=None):\n",
    "        self.containerize(line, cell)\n",
    "        ip = get_ipython()\n",
    "        ip.run_cell(cell)        \n",
    "\n",
    "    \n",
    "    @line_magic\n",
    "    def docker_build(self, line=''):\n",
    "        tokens = line.split(' ')\n",
    "        assert len(tokens) >= 1, 'Too few arguments, expecting at least [container-name]'\n",
    "        container = self.get_container(tokens[0])\n",
    "        assert container is not None, 'No container defined: {}'.format(tokens[0])\n",
    "        container.build(tokens[1:], use_gcp=False)\n",
    "        \n",
    "        \n",
    "    @line_magic\n",
    "    def containerize_file(self, line=''):\n",
    "        tokens = line.split(' ')\n",
    "        assert len(tokens) == 3, 'Expecting 3 arguments, got {}'.format(str(len(tokens)))\n",
    "        container_name, source_file, dest_file = tokens[0], tokens[1], tokens[2]\n",
    "        \n",
    "        container = self.get_container(container_name)\n",
    "        if not container:\n",
    "            container = self.Container(container_name)\n",
    "            \n",
    "        container.add_from_file(source_file, dest_file)\n",
    "        self.update_or_add_container(container_name, container)\n",
    "        \n",
    "\n",
    "    @line_magic\n",
    "    def cloud_build(self, line=''):\n",
    "        tokens = line.split(' ')\n",
    "        assert len(tokens) >= 1, 'Too few arguments, expecting at least [container-name]'\n",
    "        container = self.get_container(tokens[0])\n",
    "        assert container is not None, 'No container defined: {}'.format(tokens[0])\n",
    "        container.build(tokens[1:], use_gcp=True)\n",
    "        \n",
    "        \n",
    "    @line_magic\n",
    "    def push_container(self, line=''):\n",
    "        tokens = line.split(' ')\n",
    "        assert len(tokens) >= 1, 'Too few arguments, expecting at least [container-name]'\n",
    "        container = self.get_container(tokens[0])\n",
    "        assert container is not None, 'No container defined: {}'.format(tokens[0])\n",
    "        container.push(tokens[1:])    \n",
    "        \n",
    "        \n",
    "    @line_magic\n",
    "    def run_container(self, line=''):\n",
    "        tokens = line.split(' ')\n",
    "        assert len(tokens) >= 1, 'Too few arguments, expecting at least [container-name]'    \n",
    "        container = self.get_container(tokens[0])\n",
    "        assert container is not None, 'No container defined: {}'.format(tokens[0])\n",
    "        container.run(tokens[1:])\n",
    " \n",
    "\n",
    "ip = get_ipython()\n",
    "ip.register_magics(ContainerMagics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magic testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we do any real work, let's test our magics.  The `%%containerize_and_run` cell magic adds the cells contents to a specified file within a container, and additionally runs it in the notebook.  To _only_ add the file to a container and not run it in the notebook, use `%%containerize`.\n",
    "\n",
    "Running the cell below should print `hello there world!`.  At the same time, it's added as a file to container, ready to be built later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%containerize_and_run gcr.io/wwoo-gcp/hello-world main.py\n",
    "if __name__ == '__main__':\n",
    "    print('hello there world!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's turn this into a container.  To do so, we need a Dockerfile.  Adding a Dockerfile is just like adding any other file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%containerize gcr.io/wwoo-gcp/hello-world Dockerfile\n",
    "FROM python:3\n",
    "COPY main.py /\n",
    "ENTRYPOINT [ \"python\", \"/main.py\" ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `%cloud_build` line magic submits a container build job to Cloud Build using your `$GOOGLE_APPLICATION_CREDENTIALS`.  In Kubeflow, it uses the `user-gcp-sa` secret.\n",
    "\n",
    "`%docker_build` runs `docker build` locally to build your container image.  We don't use that here - and Jupyterlab within Kubeflow doesn't have docker installed anyway."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cloud_build gcr.io/wwoo-gcp/hello-world"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Pubfig Dataset\n",
    "\n",
    "First, let's download a suitable dataset.  We'll use \"eval\" dataset from PubFig, since this appears to have lots of example images of individual people.\n",
    "\n",
    "Let's use the `%%containerize_and_run` magic to define some code that we can both run in the notebook (for testing), in addition to being containerisable.  The download code basically reads an input file or URLs (which we'll get from the PubFig site) and spawns a number of download threads. This runs as a single container in the Kubeflow cluster.  \n",
    "\n",
    "An alternative (maybe better) approach could be to parallelise this as a Beam pipeline in Dataflow, but here we're just demonstrating how to run this within the Kubeflow cluster.  An example of running a Dataflow job (to convert the labelled images into TFRecords) is lower in the notebook. :)\n",
    "\n",
    "For each image, we're interested in the face only.  Luckily, PubFig supplies the face coordinates for each image, so we'll use that to produce crops of all the faces.  We'll use PIL to do the image manipulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install Pillow==5.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%containerize_and_run gcr.io/wwoo-gcp/pubfig-download downloader.py\n",
    "\n",
    "import logging\n",
    "import os\n",
    "import socket\n",
    "import threading\n",
    "import urllib3\n",
    "from queue import Queue\n",
    "from PIL import Image\n",
    "\n",
    "NUM_THREADS = 4\n",
    "URL_TIMEOUT = 4\n",
    "IMAGE_CROP = True \n",
    "ESCAPE_SPACES = False\n",
    "\n",
    "\n",
    "class LabelWriterThread(threading.Thread):\n",
    "    def __init__(self, queue, dest_dir):\n",
    "        super(LabelWriterThread, self).__init__()\n",
    "        self.queue = queue\n",
    "        self.daemon = True\n",
    "        self.dest_dir = dest_dir\n",
    "\n",
    "    def run(self):\n",
    "        file_path = os.path.join(self.dest_dir, \"manifest.txt\")\n",
    "        with open(file_path, 'w') as f:\n",
    "            while True:\n",
    "                f.write(self.queue.get() + '\\n')\n",
    "                f.flush()\n",
    "                self.queue.task_done()\n",
    "\n",
    "        \n",
    "class DownloadThread(threading.Thread):\n",
    "    def __init__(self, url_queue, print_queue, classes, image_crop, dest_dir):\n",
    "        super(DownloadThread, self).__init__()\n",
    "        socket.setdefaulttimeout(URL_TIMEOUT)\n",
    "        self.http = urllib3.PoolManager()\n",
    "        self.url_queue = url_queue\n",
    "        self.classes = classes\n",
    "        self.dest_dir = dest_dir\n",
    "        self.daemon = True\n",
    "        self.image_crop = image_crop\n",
    "        self.print_queue = print_queue\n",
    "\n",
    "    def run(self):\n",
    "        while True:\n",
    "            dict = self.url_queue.get()\n",
    "            \n",
    "            try:\n",
    "                name = dict[\"url\"].split('/')[-1]\n",
    "                person_dir = os.path.join(self.dest_dir, dict[\"rel_dir\"])\n",
    "                \n",
    "                try:\n",
    "                    if not os.path.exists(person_dir):\n",
    "                        os.makedirs(person_dir)\n",
    "                except Exception as e:\n",
    "                    logging.info('[%s] Path exist already.')\n",
    "\n",
    "                dest_file = os.path.join(person_dir, name)\n",
    "                self.download_url(dest_file, dict[\"url\"])\n",
    "\n",
    "                if os.path.isfile(dest_file):\n",
    "                    if self.image_crop:\n",
    "                \n",
    "                        crop_dir = os.path.join(person_dir, \"crop\")\n",
    "\n",
    "                        if not os.path.exists(crop_dir):\n",
    "                            os.makedirs(crop_dir)\n",
    "\n",
    "                        out_filename = os.path.join(crop_dir, 'crop_' + name)\n",
    "\n",
    "                        self.crop_image(dest_file, out_filename, dict[\"crop_dims\"])\n",
    "\n",
    "                        if ESCAPE_SPACES:\n",
    "                            out_filename = out_filename.replace(' ', '\\ ')\n",
    "                        \n",
    "                        label = str(self.classes[dict[\"rel_dir\"]])\n",
    "                        self.print_queue.put(out_filename + '|' + label)\n",
    "                        \n",
    "                    else:\n",
    "                        if ESCAPE_SPACES:\n",
    "                            dest_file = dest_file.replace(' ', '\\ ')\n",
    "\n",
    "                        self.print_queue.put(dest_file)\n",
    "                \n",
    "                \n",
    "            except Exception as e:\n",
    "                logging.error(\"[%s] Error: %s\" % (self.ident, e))\n",
    "           \n",
    "            finally:\n",
    "                self.url_queue.task_done()\n",
    "                \n",
    "\n",
    "    def download_url(self, dest_file, url):\n",
    "        try:\n",
    "            logging.info(\"[%s] Downloading %s -> %s\" % (self.ident, url, dest_file))\n",
    "            response = self.http.request('GET', url, preload_content=False)\n",
    "            with open(dest_file, \"wb\") as f:\n",
    "                while True:\n",
    "                    data = response.read(256)\n",
    "                    if not data:\n",
    "                        break    \n",
    "                    f.write(data)\n",
    "            \n",
    "        except urllib3.exceptions.HTTPError as e:\n",
    "            logging.error(\"[%s] HTTP Error: %s, %s\" % (self.ident, str(e), url))\n",
    "\n",
    "\n",
    "    def crop_image(self, dest_file, crop_file, crop_dims):\n",
    "        logging.info(\"[%s] Cropping %s -> %s\" % (self.ident, dest_file, crop_file))\n",
    "        c = crop_dims.split(',')\n",
    "        img = Image.open(dest_file)\n",
    "        img2 = img.crop((float(c[0]), float(c[1]), float(c[2]), float(c[3])))\n",
    "        img2.save(crop_file)\n",
    "\n",
    "        \n",
    "def read_url_file(file_path):\n",
    "    f = open(file_path)\n",
    "    queue = Queue()\n",
    "    classes = {}\n",
    "\n",
    "    for line in f:\n",
    "        if not line.startswith('#'):\n",
    "            tokens = line.split('\\t')\n",
    "            queue.put({ \"rel_dir\": tokens[0], \"url\": tokens[2], \"crop_dims\": tokens[3]})\n",
    "            if not tokens[0] in classes.keys():\n",
    "                classes[tokens[0]] = len(classes)\n",
    "                # print(str(tokens[0]) + ' ' + str(classes[tokens[0]]))\n",
    "\n",
    "    f.close()\n",
    "    return queue, classes\n",
    "\n",
    "\n",
    "def write_class_file(classes, file):\n",
    "    with open(file, 'w') as f:\n",
    "        for key in classes:\n",
    "            f.write(key + \"\\n\")\n",
    "            f.flush()\n",
    "\n",
    "   \n",
    "def start_download(url_file, dest_dir):\n",
    "    class_file = os.path.join(dest_dir, \"classes.txt\")\n",
    "    url_queue, classes = read_url_file(url_file)\n",
    "    \n",
    "    write_class_file(classes, class_file)\n",
    "    print_queue = Queue()\n",
    "\n",
    "    for i in range(NUM_THREADS):\n",
    "        t = DownloadThread(url_queue, print_queue, classes, IMAGE_CROP, dest_dir)\n",
    "        t.start()\n",
    "\n",
    "    t = LabelWriterThread(print_queue, dest_dir)\n",
    "    t.start()\n",
    "\n",
    "    url_queue.join()\n",
    "    print_queue.join()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the notebook, we can just call `start_download` with some appropriate arguments.  We'll actually do that later on, just to show it works.\n",
    "\n",
    "In a container, we need to pass some command line arguments into a python script with a main entrypoint.  So let's create a `main.py` file in the container to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%containerize gcr.io/wwoo-gcp/pubfig-download main.py\n",
    "import argparse\n",
    "import downloader\n",
    "import sys\n",
    "\n",
    "def main(argv):\n",
    "    args_parser = argparse.ArgumentParser()\n",
    "    args_parser.add_argument('--url_file', dest='url_file', required=True,\n",
    "                        help='Text file containing URLs to download.')    \n",
    "    args_parser.add_argument('--output_dir', dest='output_dir', required=True,\n",
    "                        help='Output directory.')\n",
    "    args = args_parser.parse_args(argv)\n",
    "    \n",
    "    downloader.start_download(args.url_file, args.output_dir)    \n",
    "    \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(sys.argv[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%containerize gcr.io/wwoo-gcp/pubfig-download __init__.py\n",
    "## Empty file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a simple wrapper bash script as the entrypoint to the container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%containerize gcr.io/wwoo-gcp/pubfig-download entrypoint.sh\n",
    "#!/bin/bash\n",
    "BUCKET=$1\n",
    "echo == Bucket is $BUCKET ==\n",
    "mkdir -p /tmp/pubfig_eval\n",
    "python3 /main.py --url_file /eval_urls.txt --output_dir /tmp/pubfig_eval\n",
    "sed -i \"s/\\/tmp\\/pubfig_eval\\//gs\\:\\/\\/${BUCKET}\\/pubfig_eval\\//\" /tmp/pubfig_eval/manifest.txt\n",
    "gsutil -m cp -R /tmp/pubfig_eval gs://${BUCKET}/pubfig_eval\n",
    "echo \"gs://${BUCKET}/pubfig_eval/manifest.txt\" > /output.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the dataset URL file from the PubFig site.  Another approach would be to download the file when the container is run, but here we're going to just include the file inside the container image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "echo [ Downloading PubFig url file and create a sample subset ... ]\n",
    "curl -O http://www.cs.columbia.edu/CAVE/databases/pubfig/download/eval_urls.txt\n",
    "tail -n 200 eval_urls.txt > sample_urls.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we've downloaded the file and it exists in the local file system, we can just use `%containerize_file` to add it to a container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%containerize_file gcr.io/wwoo-gcp/pubfig-download eval_urls.txt eval_urls.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%containerize_file gcr.io/wwoo-gcp/pubfig-download sample_urls.txt sample_urls.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add a `requirements.txt` file for the python dependencies, using `%%containerize` to add the cell contents to a container file.  Then define a Dockerfile and use Cloud Build to build the container!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%containerize gcr.io/wwoo-gcp/pubfig-download requirements.txt\n",
    "urllib3==1.24.1\n",
    "Pillow==5.3.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%containerize gcr.io/wwoo-gcp/pubfig-download Dockerfile\n",
    "FROM google/cloud-sdk:latest\n",
    "COPY *.py /\n",
    "COPY eval_urls.txt /\n",
    "COPY sample_urls.txt /\n",
    "COPY entrypoint.sh /\n",
    "COPY requirements.txt /\n",
    "RUN apt-get -y install python3-pip\n",
    "RUN pip3 install -r requirements.txt\n",
    "ENTRYPOINT [ \"bash\", \"/entrypoint.sh\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cloud_build gcr.io/wwoo-gcp/pubfig-download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we used `%%containerize_and_run`, the `start_download` function is defined in the notebook.  We can download the dataset within the notebook, or launch the container we just built to run the code.  For example, we could include it in the Kubeflow pipeline - which we do later on!.\n",
    "\n",
    "In the following cells, we'll try calling `start_download` as a test of running some code that has been containerised, but also callable in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ Installing python dependencies ... ]\n",
      "[ Removing old files ... ]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "echo [ Installing python dependencies ... ]\n",
    "#pip3 install urllib3==1.24.1\n",
    "#pip3 install Pillow==5.3.0\n",
    "\n",
    "echo [ Removing old files ... ]\n",
    "rm -rf /tmp/pubfig\n",
    "mkdir -p /tmp/pubfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There will be lots of ERRORs, as the url file contains lots of broken links.  Nothing we can do there - but we'll have some data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_download('eval_urls.txt', '/tmp/pubfig')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see the output. The `manifest.txt` has a written record for each image successfully downloaded and cropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/tmp/pubfig/William Macy/crop/crop_wildhogsprempic33.jpg|139\n",
      "/tmp/pubfig/William Macy/crop/crop_wlcl5b.jpg|139\n",
      "/tmp/pubfig/William Macy/crop/crop_207526936_d9d5d4a75e_o.jpg|139\n",
      "/tmp/pubfig/William Macy/crop/crop_243909925_1aeba9954c_o.jpg|139\n",
      "/tmp/pubfig/William Macy/crop/crop_245712735_4d83608e75_o.jpg|139\n",
      "/tmp/pubfig/William Macy/crop/crop_39937487_72abc68cc2_o.jpg|139\n",
      "/tmp/pubfig/William Macy/crop/crop_39937508_3fec58c312_o.jpg|139\n",
      "/tmp/pubfig/William Macy/crop/crop_243909925_1aeba9954c_o.jpg|139\n",
      "/tmp/pubfig/William Macy/crop/crop_244004030_4711d5f04b_o.jpg|139\n",
      "/tmp/pubfig/William Macy/crop/crop_23878732_579d04d823_o.jpg|139\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# ls -R /tmp/pubfig\n",
    "tail /tmp/pubfig/manifest.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run a Beam job on Dataflow to convert the dataset into TFRecords.  The job will read a list of image paths and labels from a CSV file, then output TFRecords which we'll use for training, evaluation and prediction later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%containerize gcr.io/wwoo-gcp/df-preproc Dockerfile\n",
    "FROM gcr.io/ml-pipeline/ml-pipeline-dataflow-tft:latest    \n",
    "RUN mkdir /app\n",
    "COPY main.py /app\n",
    "COPY setup.py /app\n",
    "RUN mkdir /data\n",
    "COPY sample.txt /data\n",
    "ENTRYPOINT [\"python\", \"/app/main.py\", \"--setup_file\", \"/app/setup.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample snippet of the input CSV. We'll include it in the container image using our custom magics, just as a demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%containerize gcr.io/wwoo-gcp/df-preproc sample.txt\n",
    "gs://wwoo-pubfig/data/Alicia Keys/crop/crop_040830_keys_vmed_1p.widec.jpg|0\n",
    "gs://wwoo-pubfig/data/Alicia Keys/crop/crop_081608alicia.jpg|0\n",
    "gs://wwoo-pubfig/data/Alicia Keys/crop/crop_100_alicia_keys.jpg|0\n",
    "gs://wwoo-pubfig/data/Alicia Keys/crop/crop_1023041_Alicia%20Keys9.jpg|0\n",
    "gs://wwoo-pubfig/data/Alicia Keys/crop/crop_111708alicia.jpg|0\n",
    "gs://wwoo-pubfig/data/John Travolta/crop/crop_john_travolta_004_wenn1460435.jpg|25\n",
    "gs://wwoo-pubfig/data/John Travolta/crop/crop_john_travolta_06-02.jpg|25\n",
    "gs://wwoo-pubfig/data/John Travolta/crop/crop_john_travolta_12215142.jpg|25\n",
    "gs://wwoo-pubfig/data/John Travolta/crop/crop_john_travolta_1734222.jpg|25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python setup file of job dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%containerize gcr.io/wwoo-gcp/df-preproc setup.py\n",
    "from setuptools import find_packages\n",
    "from setuptools import setup\n",
    "\n",
    "REQUIRED_PACKAGES = ['Pillow==5.3.0']\n",
    "\n",
    "setup(\n",
    "    name='df-preproc',\n",
    "    version='0.1',\n",
    "    install_requires=REQUIRED_PACKAGES,\n",
    "    packages=find_packages(),\n",
    "    include_package_data=True,\n",
    "    description=''\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code for the beam job is all in a file called `main.py`, to make things easier for our cell to file mapping.  We could split things into multiple files too, by using separate cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%containerize gcr.io/wwoo-gcp/df-preproc main.py\n",
    "\n",
    "import apache_beam as beam\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "\n",
    "class ReadImagesAndConvertToJpegDoFn(beam.DoFn):\n",
    "    def __init__(self, prepend_path=None, image_width=100, image_height=100):\n",
    "        self.prepend_path = prepend_path\n",
    "        self.image_width = image_width\n",
    "        self.image_height = image_height\n",
    "\n",
    "\n",
    "    def process(self, elem):\n",
    "        # local scoped imports\n",
    "        import io\n",
    "        import numpy as np\n",
    "        from PIL import Image\n",
    "\n",
    "        key, uri = elem\n",
    "        file_path = uri if self.prepend_path is None else os.path.join(self.prepend_path, uri)\n",
    "\n",
    "        # TF will enable 'rb' in future versions, but until then, 'r' is\n",
    "        # required.\n",
    "        def _open_file_read_binary(uri):\n",
    "            # local scoped imports\n",
    "            import os\n",
    "            from tensorflow.python.framework import errors\n",
    "            from tensorflow.python.lib.io import file_io\n",
    "\n",
    "            try:\n",
    "                return file_io.FileIO(file_path, mode='rb')\n",
    "            except errors.InvalidArgumentError:\n",
    "                return file_io.FileIO(file_path, mode='r')\n",
    "\n",
    "        try:\n",
    "            img_raw = []\n",
    "            \n",
    "            with _open_file_read_binary(file_path) as f:\n",
    "                image_bytes = f.read()\n",
    "                image_pil = Image.open(io.BytesIO(image_bytes)).convert('RGB')\n",
    "                image_pil = image_pil.resize((self.image_width, self.image_height))\n",
    "                img = np.array(image_pil)\n",
    "                img_raw.append(img.tostring())\n",
    "\n",
    "            # A variety of different calling libraries throw different exceptions here.\n",
    "            # They all correspond to an unreadable file so we treat them equivalently.\n",
    "        except Exception as e:  # pylint: disable=broad-except\n",
    "            logging.exception('Error processing image %s: %s', uri, str(e))\n",
    "            return\n",
    "\n",
    "        # key is the class label\n",
    "        # value is a dict containing the raw image bytes and source image path\n",
    "        yield (key, { 'img_raw': img_raw, 'file_path': file_path })\n",
    "\n",
    "        \n",
    "class CreateTFRecords(beam.PTransform):\n",
    "\n",
    "    def __init__(self, out_path, dataset_name):\n",
    "        super(CreateTFRecords, self).__init__()\n",
    "        self.out_path = out_path\n",
    "        self.dataset_name = dataset_name\n",
    "\n",
    "    def expand(self, pcol):\n",
    "        import os\n",
    "\n",
    "        class TFExampleDoFn(beam.DoFn):\n",
    "            def process(self, elem):\n",
    "                # local scoped imports\n",
    "                import tensorflow as tf\n",
    "\n",
    "                def _bytes_feature(value):\n",
    "                    return tf.train.Feature(bytes_list=tf.train.BytesList(value=value))\n",
    "\n",
    "                def _int64_feature(value):\n",
    "                    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "                \n",
    "                def _str_feature(value):\n",
    "                    return tf.train.Feature(int64_list=tf.train.Int64List(value=value))\n",
    "\n",
    "                key, value = elem\n",
    "                example = tf.train.Example(features=tf.train.Features(feature={\n",
    "                    'bytes': _bytes_feature(value['img_raw']),\n",
    "                    'path': _bytes_feature([value['file_path'].encode('utf8')]),\n",
    "                    'label': _int64_feature([int(key)])\n",
    "                }))\n",
    "\n",
    "                yield example\n",
    "\n",
    "        (pcol\n",
    "            | 'ImageToExample' >> beam.ParDo(TFExampleDoFn())\n",
    "            | 'SerializeToString' >> beam.Map(lambda x: x.SerializeToString())\n",
    "            | 'WriteTFRecord' >> beam.io.WriteToTFRecord(\n",
    "                os.path.join(self.out_path, self.dataset_name),\n",
    "                file_name_suffix='.tfrecord.gz'))   \n",
    "        \n",
    "def run_pipeline(p, opt=None):\n",
    "    import random\n",
    "\n",
    "    def _add_keys(x):\n",
    "        t = x.split('|')\n",
    "        return (t[1], t[0])\n",
    "\n",
    "    def _unwind_samples((key, values)):\n",
    "        for v in values:\n",
    "            yield (key, v)\n",
    "\n",
    "    all_data = (p | 'ReadInputCSV' >> beam.io.ReadFromText(opt.input_csv))\n",
    "    train_set, eval_set = (all_data\n",
    "        | 'AddKeys' >> beam.Map(_add_keys)\n",
    "        | 'ReadImagesAndConvertToJpeg' >> beam.ParDo(ReadImagesAndConvertToJpegDoFn(\n",
    "            prepend_path=opt.prepend_path,\n",
    "            image_width=opt.image_width,\n",
    "            image_height=opt.image_height))\n",
    "        | 'GroupByKey' >> beam.GroupByKey()\n",
    "        | 'UnwindSamples' >> beam.FlatMap(_unwind_samples)\n",
    "        | 'SplitDataset' >> beam.Partition(\n",
    "            lambda x, _: int(random.uniform(0, 100) < int(opt.eval_percent)), 2))\n",
    "\n",
    "    train_set | 'CreateTrainSet' >> CreateTFRecords(out_path=opt.out_path, dataset_name='train')\n",
    "\n",
    "    eval_set | 'CreateEvalSet' >> CreateTFRecords(out_path=opt.out_path, dataset_name='eval')\n",
    "\n",
    "    p.run().wait_until_finish() \n",
    "\n",
    "\n",
    "def main(argv):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--input_csv', dest='input_csv', required=True,\n",
    "                        help='Input CSV file.')\n",
    "    parser.add_argument('--out_path', dest='out_path', required=True,\n",
    "                        help='Output path location for TFRecord files.')\n",
    "    parser.add_argument('--path_prefix', dest='path_prefix',\n",
    "                        help='Image path prefix for reading source image files.')\n",
    "    parser.add_argument('--image_width', dest='image_width', type=int,\n",
    "                        help='Image width resize', default=64)\n",
    "    parser.add_argument('--image_height', dest='image_height', type=int,\n",
    "                        help='Image height resize', default=64)\n",
    "    parser.add_argument('--prepend_path', dest='prepend_path',\n",
    "                        help='Path to prepend for reading images')\n",
    "    parser.add_argument('--eval_percent', dest='eval_percent',\n",
    "                        help='Percentage of dataset to use for evaluation', required=True)\n",
    "    known_args, pipeline_args = parser.parse_known_args(argv)\n",
    "\n",
    "    p = beam.Pipeline(argv=pipeline_args)\n",
    "    run_pipeline(p, known_args)\n",
    "    \n",
    "    # the next step of pipeline will look for this file\n",
    "    with open(\"/output.txt\", \"w\") as output_file:\n",
    "        output_file.write(known_args.out_path)\n",
    "        print(\"Done!\")       \n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main(sys.argv[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `%cloud_build` line magic submits a container build job to Cloud Build."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the container\n",
    "%cloud_build gcr.io/wwoo-gcp/df-preproc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate metadata for embedding projector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.24.1) or chardet (3.0.4) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import numpy as np\n",
    "import math\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def generate_sprite_image(array, ncols=30):\n",
    "    nindex, height, width, channels = array.shape\n",
    "    nrows = math.ceil(nindex/ncols)\n",
    "    result = np.zeros((height*nrows, width*ncols, channels), dtype=array.dtype)\n",
    "    \n",
    "    for i in range(0, nrows):\n",
    "        for j in range(0, ncols):\n",
    "            sprite_num = i*ncols+j\n",
    "            if (sprite_num >= nindex):\n",
    "                break\n",
    "            result[i*height:(i+1)*height, j*width:(j+1)*width, :] = array[sprite_num]\n",
    "    \n",
    "    print('Total sprites: {}'.format(str(nindex)))\n",
    "    \n",
    "    return result        \n",
    "    \n",
    "    \n",
    "def make_meta(input_path, output_dir):\n",
    "    def parser(example_proto):\n",
    "        features = {\n",
    "            \"bytes\": tf.FixedLenFeature((), tf.string, default_value=\"\"),\n",
    "            \"label\": tf.FixedLenFeature((), tf.int64, default_value=0),\n",
    "            \"path\": tf.FixedLenFeature((), tf.string, default_value='')\n",
    "        }\n",
    "        parsed_features = tf.parse_single_example(example_proto, features)\n",
    "        image = tf.decode_raw(parsed_features[\"bytes\"], tf.uint8)\n",
    "        # image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "        return image, parsed_features[\"label\"], parsed_features['path']\n",
    "    \n",
    "    sprite_array = []\n",
    "    sprite_image = None\n",
    "    \n",
    "    image = None\n",
    "    \n",
    "    with tf.Session() as sess:        \n",
    "        \n",
    "        filenames = tf.matching_files(input_path)\n",
    "\n",
    "        dataset = tf.data.TFRecordDataset(filenames=filenames,\n",
    "                compression_type=\"GZIP\")\n",
    "        dataset = dataset.map(parser)\n",
    "\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        _image, _label, _path = iterator.get_next()\n",
    "        \n",
    "        with open(os.path.join(output_dir, 'meta.tsv'), 'wb') as f:        \n",
    "            while True:\n",
    "                try:\n",
    "                    image, label, path = sess.run([_image, _label, _path])\n",
    "                    f.write((str(label)+'\\n').encode(\"utf-8\"))\n",
    "                    sprite_array.append(image.reshape([64, 64, 3]))\n",
    "\n",
    "                except tf.errors.OutOfRangeError:\n",
    "                    sprite_image = generate_sprite_image(np.asarray(sprite_array))\n",
    "                    img = Image.fromarray(sprite_image, 'RGB')\n",
    "                    img.save(os.path.join(output_dir, 'sprites.png'))\n",
    "                    break\n",
    "                    \n",
    "            f.flush()\n",
    "            f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_meta('gs://wwoo-pubfig/tfrecords/eval-00000-of-*.tfrecord.gz', '/tmp')\n",
    "Image.open('/tmp/sprites.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is far from perfect.  We can see there's a lot of non-faces here.\n",
    "\n",
    "<img src=\"sprites.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying file:///tmp/sprites.png [Content-Type=image/png]...\n",
      "/ [1 files][  6.2 MiB/  6.2 MiB]                                                \r\n",
      "Operation completed over 1 objects/6.2 MiB.                                      \n",
      "Copying file:///tmp/meta.tsv [Content-Type=text/tab-separated-values]...\n",
      "/ [1 files][  2.8 KiB/  2.8 KiB]                                                \r\n",
      "Operation completed over 1 objects/2.8 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "gsutil cp /tmp/sprites.png gs://wwoo-pubfig/embeddings\n",
    "gsutil cp /tmp/meta.tsv gs://wwoo-pubfig/embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Container for our training code.  This is scheduled by Kubeflow pipelines like any other container and doesn't use TFJobs (that would be a nice TODO)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%containerize gcr.io/wwoo-gcp/triplet-loss Dockerfile\n",
    "FROM gcr.io/ml-pipeline/ml-pipeline-kubeflow-trainer:latest\n",
    "RUN mkdir -p /app/meta\n",
    "COPY task.py /app\n",
    "RUN pip install tensorflow-hub>=0.1.1\n",
    "ENTRYPOINT [\"python\", \"/app/task.py\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%containerize gcr.io/wwoo-gcp/triplet-loss task.py\n",
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "\n",
    "def initialize_hyper_params(args):\n",
    "    args_parser = argparse.ArgumentParser()\n",
    "    \n",
    "    args_parser.add_argument(\n",
    "        '--margin',\n",
    "        help='Triplet loss margin',\n",
    "        type=float,\n",
    "        default=1.0)\n",
    "\n",
    "    args_parser.add_argument(\n",
    "        '--learning-rate',\n",
    "        help='Learning rate',\n",
    "        type=float,\n",
    "        default=0.0001)\n",
    "\n",
    "    args_parser.add_argument(\n",
    "        '--input-files',\n",
    "        help='Path to input files',\n",
    "        type=str,\n",
    "        required=True)\n",
    "\n",
    "    args_parser.add_argument(\n",
    "        '--batch-size',\n",
    "        help='Batch size',\n",
    "        type=int,\n",
    "        default=200)\n",
    "    \n",
    "    args_parser.add_argument(\n",
    "        '--image-size',\n",
    "        help='Image size',\n",
    "        type=int,\n",
    "        default=299)    \n",
    "\n",
    "    args_parser.add_argument(\n",
    "        '--train-steps',\n",
    "        help='Training steps',\n",
    "        type=int,\n",
    "        default=200)\n",
    "\n",
    "    args_parser.add_argument(\n",
    "        '--model-dir',\n",
    "        help='Model directory',\n",
    "        type=str,\n",
    "        required=True)\n",
    "\n",
    "    args_parser.add_argument(\n",
    "        '--save-summary-steps',\n",
    "        help='Number of steps before saving summary',\n",
    "        type=int,\n",
    "        default=100)\n",
    "\n",
    "    args_parser.add_argument(\n",
    "        '--num-epochs',\n",
    "        help='Number of training epochs',\n",
    "        type=int,\n",
    "        default=None)     \n",
    "    \n",
    "    args_parser.add_argument(\n",
    "        '--max-predictions',\n",
    "        help='Maximum number of predictions',\n",
    "        type=int,\n",
    "        default=None)    \n",
    "    \n",
    "    args_parser.add_argument(\n",
    "        '--embedding-dir',\n",
    "        help='Predicted embeddings directory',\n",
    "        type=str,\n",
    "        required=False)    \n",
    "       \n",
    "    args_parser.add_argument(\n",
    "        '--verbosity',\n",
    "        choices=[\n",
    "            'DEBUG',\n",
    "            'ERROR',\n",
    "            'FATAL',\n",
    "            'INFO',\n",
    "            'WARN'\n",
    "        ],\n",
    "        default='INFO',\n",
    "    )\n",
    "    \n",
    "    args_parser.add_argument(\n",
    "        '--mode',\n",
    "        choices=[\n",
    "            'train',\n",
    "            'evaluate',\n",
    "            'predict'\n",
    "        ],\n",
    "        default='train',\n",
    "    )\n",
    "\n",
    "    return args_parser.parse_args(args)\n",
    "\n",
    "\n",
    "HYPER_PARAMS = None\n",
    "\n",
    "\n",
    "def parser(example_proto):\n",
    "    features = {\n",
    "        \"bytes\": tf.FixedLenFeature((), tf.string, default_value=\"\"),\n",
    "        \"path\": tf.FixedLenFeature((), tf.string, default_value=\"\"),        \n",
    "        \"label\": tf.FixedLenFeature((), tf.int64, default_value=0)\n",
    "    }\n",
    "    parsed_features = tf.parse_single_example(example_proto, features)\n",
    "    image = tf.decode_raw(parsed_features[\"bytes\"], tf.uint8)\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "    return image, parsed_features[\"label\"]\n",
    "\n",
    "\n",
    "def generate_input_fn(file_pattern,\n",
    "                      shuffle=True,\n",
    "                      num_epochs=1,\n",
    "                      batch_size=200,\n",
    "                      multi_threading=True):\n",
    "\n",
    "    def _input_fn():\n",
    "        file_names = tf.matching_files(file_pattern)\n",
    "        buffer_size = 2 * batch_size + 1\n",
    "\n",
    "        dataset = tf.data.TFRecordDataset(filenames=file_names,\n",
    "            compression_type=\"GZIP\")\n",
    "        dataset = dataset.map(parser)\n",
    "\n",
    "        if shuffle:\n",
    "            dataset = dataset.shuffle(buffer_size)\n",
    "\n",
    "        dataset = dataset.batch(batch_size)\n",
    "        dataset = dataset.prefetch(buffer_size)\n",
    "        dataset = dataset.repeat(num_epochs)\n",
    "\n",
    "        iterator = dataset.make_one_shot_iterator()\n",
    "        features, labels = iterator.get_next()\n",
    "\n",
    "        return features, labels\n",
    "\n",
    "    return _input_fn\n",
    "\n",
    "\n",
    "def build_model(images, is_training=True):\n",
    "    images = tf.image.resize_images(images, (299, 299))    \n",
    "    images = tf.map_fn(lambda x: tf.image.per_image_standardization(x), images, parallel_iterations=10)\n",
    "    m = hub.Module(\"https://tfhub.dev/google/imagenet/inception_v3/feature_vector/1\",\n",
    "        tags={\"train\"}, trainable=is_training)\n",
    "    embeddings = m(images)\n",
    "    return tf.nn.l2_normalize(embeddings, axis=1)\n",
    "\n",
    "\n",
    "def model_fn(features, labels, mode):\n",
    "    is_training = (mode == tf.estimator.ModeKeys.TRAIN)\n",
    "    images = tf.reshape(features, [-1, HYPER_PARAMS.image_size, HYPER_PARAMS.image_size, 3])\n",
    "    assert images.shape[1:] == [HYPER_PARAMS.image_size, HYPER_PARAMS.image_size, 3], \"{}\".format(images.shape)\n",
    "\n",
    "    # trainable = True if mode == tf.estimator.ModeKeys.TRAIN else False \n",
    "    trainable = True\n",
    "    \n",
    "    with tf.variable_scope('model'):\n",
    "        embeddings = build_model(images, trainable) \n",
    "        assert embeddings.shape[1:] == [2048], \"{}\".format(embeddings.shape)\n",
    "    \n",
    "    if mode == tf.estimator.ModeKeys.TRAIN or mode == tf.estimator.ModeKeys.EVAL:\n",
    "        loss = tf.contrib.losses.metric_learning.triplet_semihard_loss(\n",
    "            labels=labels, embeddings=embeddings, margin=HYPER_PARAMS.margin)\n",
    "\n",
    "        if mode == tf.estimator.ModeKeys.EVAL:\n",
    "            return tf.estimator.EstimatorSpec(mode=mode, loss=loss)\n",
    "        \n",
    "        # Minimize loss\n",
    "        optimizer = tf.train.AdamOptimizer(HYPER_PARAMS.learning_rate)\n",
    "        global_step = tf.train.get_global_step()\n",
    "        train_op = optimizer.minimize(loss, global_step=global_step)     \n",
    "        \n",
    "        # Provide an estimator spec for `ModeKeys.EVAL` and `ModeKeys.TRAIN` modes.\n",
    "        return tf.estimator.EstimatorSpec(mode=mode,\n",
    "                                          train_op=train_op,\n",
    "                                          loss=loss)\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        predictions = {'embeddings': embeddings}\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "    global HYPER_PARAMS\n",
    "    HYPER_PARAMS = initialize_hyper_params(argv)\n",
    "\n",
    "    tf.logging.set_verbosity(HYPER_PARAMS.verbosity)\n",
    "    \n",
    "    config = tf.estimator.RunConfig(tf_random_seed=3423458,\n",
    "                                    model_dir=HYPER_PARAMS.model_dir,\n",
    "                                    save_summary_steps=HYPER_PARAMS.save_summary_steps)\n",
    "\n",
    "    estimator = tf.estimator.Estimator(model_fn=model_fn, config=config)\n",
    "    \n",
    "    if HYPER_PARAMS.mode == 'train':\n",
    "        my_input_fn = generate_input_fn(file_pattern=HYPER_PARAMS.input_files,\n",
    "                                        shuffle=True,\n",
    "                                        num_epochs=HYPER_PARAMS.num_epochs,\n",
    "                                        batch_size=HYPER_PARAMS.batch_size)  \n",
    "        \n",
    "        estimator.train(my_input_fn, max_steps=int(HYPER_PARAMS.train_steps))\n",
    "\n",
    "        # the next step of pipeline will look for this file\n",
    "        with open(\"/model_dir.txt\", \"w\") as output_file:\n",
    "            output_file.write(HYPER_PARAMS.model_dir)\n",
    "            print(\"Done!\")\n",
    "        \n",
    "        \n",
    "    elif HYPER_PARAMS.mode == 'evaluate':\n",
    "        my_input_fn = generate_input_fn(file_pattern=HYPER_PARAMS.input_files,\n",
    "                                        shuffle=False,\n",
    "                                        num_epochs=1,\n",
    "                                        batch_size=HYPER_PARAMS.batch_size)        \n",
    "        \n",
    "        estimator.evaluate(my_input_fn)\n",
    "\n",
    "        # the next step of pipeline will look for this file\n",
    "        with open(\"/output.txt\", \"w\") as output_file:\n",
    "            output_file.write('done')\n",
    "            print(\"Done!\")\n",
    "\n",
    "    elif HYPER_PARAMS.mode == 'predict':\n",
    "        my_input_fn = generate_input_fn(file_pattern=HYPER_PARAMS.input_files,\n",
    "                                        shuffle=False,\n",
    "                                        num_epochs=1,\n",
    "                                        batch_size=HYPER_PARAMS.batch_size)        \n",
    "        \n",
    "        predictions = estimator.predict(my_input_fn)\n",
    "\n",
    "        embeddings = np.zeros((HYPER_PARAMS.max_predictions, 2048))\n",
    "        for i, p in enumerate(predictions):\n",
    "            if i > HYPER_PARAMS.max_predictions - 1:\n",
    "                break\n",
    "            embeddings[i] = p['embeddings']\n",
    "        \n",
    "        with tf.Graph().as_default():\n",
    "            # Visualize test embeddings\n",
    "            embedding_var = tf.Variable(embeddings, name='face_embedding')\n",
    "\n",
    "            config = projector.ProjectorConfig()\n",
    "            embedding = config.embeddings.add()\n",
    "            embedding.tensor_name = embedding_var.name\n",
    "            embedding.sprite.image_path = 'sprites.png' # we will create this later\n",
    "            embedding.sprite.single_image_dim.extend([64, 64])    \n",
    "            embedding.metadata_path = 'meta.tsv'\n",
    "\n",
    "            summary_writer = tf.summary.FileWriter(HYPER_PARAMS.embedding_dir)        \n",
    "            projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "            saver = tf.train.Saver()\n",
    "            with tf.Session() as sess:\n",
    "                sess.run(tf.global_variables_initializer())\n",
    "                # saver = tf.train.Saver([embedding_var])\n",
    "                saver.save(sess, os.path.join(HYPER_PARAMS.embedding_dir, \"embeddings.ckpt\"))\n",
    "\n",
    "        # the next step of pipeline will look for this file\n",
    "        with open(\"/embedding_dir.txt\", \"w\") as output_file:\n",
    "            output_file.write(HYPER_PARAMS.embedding_dir)\n",
    "            print(\"Done!\")\n",
    "\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main(sys.argv[1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cloud_build gcr.io/wwoo-gcp/triplet-loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Kubeflow pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have all the containers we need already built.  All that's left to do is define a pipeline to link all the steps together!\n",
    "\n",
    "The final pipeline should look like this:\n",
    "\n",
    "<img src=\"triplet-loss-pipeline.png\" width=\"400\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#KFP_PACKAGE = 'https://storage.googleapis.com/ml-pipeline/release/0.1.3-rc.2/kfp.tar.gz'\n",
    "KFP_PACKAGE = 'https://storage.googleapis.com/ml-pipeline/release/0.1.4/kfp.tar.gz'\n",
    "\n",
    "# Install Pipeline SDK\n",
    "!pip3 install $KFP_PACKAGE --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "import kfp.dsl as dsl\n",
    "import kfp.gcp as gcp\n",
    "import os\n",
    "\n",
    "PROJECT = 'wwoo-gcp'\n",
    "REGION = 'us-central1'\n",
    "BUCKET = 'wwoo-pubfig'\n",
    "EXPERIMENT_NAME = 'triplet-learning'\n",
    "\n",
    "TRAIN_STEPS = 5000\n",
    "BATCH_SIZE = 100\n",
    "IMAGE_SIZE = 64\n",
    "\n",
    "client = kfp.Client()\n",
    "#exp = client.create_experiment(name=EXPERIMENT_NAME)\n",
    "exp = client.get_experiment('9ae33307-b66d-49c6-bc59-8b3296f30927')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ObjectDict(dict):\n",
    "    def __getattr__(self, name):\n",
    "        if name in self:\n",
    "            return self[name]\n",
    "        else:\n",
    "            raise AttributeError(\"No such attribute: \" + name)\n",
    "\n",
    "\n",
    "@dsl.pipeline(\n",
    "  name='triplet_learning',\n",
    "  description='Triplet learning pipeline'\n",
    ")\n",
    "def triplet_learning(\n",
    "    project=dsl.PipelineParam(name='project', value=PROJECT),\n",
    "    bucket=dsl.PipelineParam(name='bucket', value=BUCKET),\n",
    "    train_steps=dsl.PipelineParam(name='train-steps', value=TRAIN_STEPS),\n",
    "    batch_size=dsl.PipelineParam(name='train-batch-size', value=BATCH_SIZE),\n",
    "    image_size=dsl.PipelineParam(name='image-size', value=IMAGE_SIZE)\n",
    "):\n",
    "    start_step = 0\n",
    "    \n",
    "    if start_step == 0:\n",
    "        downloader = dsl.ContainerOp(\n",
    "            name='pubfig-download',\n",
    "            image='gcr.io/wwoo-gcp/pubfig-download:latest',\n",
    "            arguments=[\n",
    "                str(bucket)\n",
    "              ],\n",
    "            file_outputs={'manifest': '/output.txt'}\n",
    "        )\n",
    "    else:\n",
    "        downloader = ObjectDict({\n",
    "            'outputs': {\n",
    "                'manifest': os.path.join('gs://', str(bucket), 'pubfig_eval', 'manifest.txt')\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        \n",
    "    if start_step == 1:\n",
    "        preprocess = dsl.ContainerOp(\n",
    "            name='preprocess-on-df',\n",
    "            image='gcr.io/wwoo-gcp/df-preproc:latest',\n",
    "            arguments=[\n",
    "                '--project', project,\n",
    "                '--runner', 'DataflowRunner',\n",
    "                '--staging-location', os.path.join('gs://', str(bucket), 'staging'),\n",
    "                '--experiments', 'shuffle_mode=service',\n",
    "                '--eval_percent', 20,\n",
    "                '--image_width', image_size,\n",
    "                '--image_height', image_size,\n",
    "                '--temp_location', os.path.join('gs://', str(bucket), 'temp'),\n",
    "                '--job_name', 'df-preprocess',\n",
    "                '--input_csv', downloader.outputs['manifest'],\n",
    "                '--out_path', os.path.join('gs://', str(bucket), 'tfrecords')\n",
    "\n",
    "              ],\n",
    "            file_outputs={'outpath': '/output.txt'}\n",
    "        )\n",
    "    else:\n",
    "        preprocess = ObjectDict({\n",
    "            'outputs': {\n",
    "                'outpath': os.path.join('gs://', str(bucket), 'tfrecords')\n",
    "            }\n",
    "        })\n",
    "        \n",
    "    if start_step <= 2:\n",
    "        train = dsl.ContainerOp(\n",
    "            name='train',\n",
    "            image='gcr.io/wwoo-gcp/triplet-loss',\n",
    "            arguments=[\n",
    "                '--mode', 'train',\n",
    "                '--input-files', os.path.join(str(preprocess.outputs['outpath']), 'train-*.tfrecord.gz'),\n",
    "                '--batch-size', batch_size,\n",
    "                '--learning-rate', 0.001,\n",
    "                '--image-size', image_size,\n",
    "                '--train-steps', train_steps,\n",
    "                '--model-dir', os.path.join('gs://', str(bucket), 'model')\n",
    "            ],\n",
    "            file_outputs={\n",
    "                'model-dir': '/model_dir.txt',\n",
    "            }\n",
    "        )\n",
    "        train.set_memory_request('4G')\n",
    "        train.set_cpu_request('3')\n",
    "    else:\n",
    "        train = ObjectDict({\n",
    "            'outputs': {\n",
    "                'model-dir': os.path.join('gs://', str(bucket), 'model')\n",
    "            }\n",
    "        })        \n",
    "\n",
    "\n",
    "    if start_step <= 3:\n",
    "        evaluate = dsl.ContainerOp(\n",
    "            name='evalulate',\n",
    "            image='gcr.io/wwoo-gcp/triplet-loss',\n",
    "            arguments=[\n",
    "                '--mode', 'evaluate',\n",
    "                '--input-files', os.path.join(str(preprocess.outputs['outpath']), 'eval-*.tfrecord.gz'),\n",
    "                '--batch-size', batch_size,\n",
    "                '--image-size', image_size,                \n",
    "                '--model-dir', train.outputs['model-dir']\n",
    "            ],\n",
    "            file_outputs={\n",
    "                'output': '/output.txt',\n",
    "            }\n",
    "        )   \n",
    "        evaluate.set_memory_request('12G')\n",
    "        evaluate.set_cpu_request('3')     \n",
    "        \n",
    "   \n",
    "    if start_step <= 3:\n",
    "        predict_eval = dsl.ContainerOp(\n",
    "            name='predict-eval',\n",
    "            image='gcr.io/wwoo-gcp/triplet-loss',\n",
    "            arguments=[\n",
    "                '--mode', 'predict',\n",
    "                '--input-files', os.path.join(str(preprocess.outputs['outpath']), 'eval-00000-of-*.tfrecord.gz'),\n",
    "                '--max-predictions', 907,\n",
    "                '--batch-size', batch_size,                \n",
    "                '--image-size', image_size,\n",
    "                '--model-dir', train.outputs['model-dir'],\n",
    "                '--embedding-dir', os.path.join('gs://', str(bucket), 'embeddings')\n",
    "            ],\n",
    "            file_outputs={\n",
    "                'embedding-dir': '/embedding_dir.txt',\n",
    "            }\n",
    "        )    \n",
    "        predict_eval.set_memory_request('4G')\n",
    "        predict_eval.set_cpu_request('3')           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Run link <a href=\"/pipeline/#/runs/details/10ed5ca7-0277-11e9-8853-42010a800044\" target=\"_blank\" >here</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import kfp.compiler as compiler\n",
    "    \n",
    "# Compile it into a tar package.\n",
    "compiler.Compiler().compile(triplet_learning, 'triplet_learning.tar.gz')\n",
    "run = client.run_pipeline(exp.id, 'triplet_learning', 'triplet_learning.tar.gz', params={})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualise embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the experiment finishes, you can launch Tensorboard on your laptop.\n",
    "\n",
    "`tensorboard --logdir gs://YOUR_BUCKET/embeddings`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example T-SNE visualization of the evaluation set.  This is one of the better looking \"clusters\".\n",
    "\n",
    "\n",
    "<img src=\"t-sne-triplets-eval.png\" width=\"800\"/>\n",
    "\n",
    "\n",
    "\n",
    "Example T-SNE visualization of a (scrubbed!) training set.  I removed all the non-face images, and used Cloud Vision API to crop the faces instead of using the provided face coordinates.\n",
    "\n",
    "\n",
    "<img src=\"t-sne-triplets-train.png\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
